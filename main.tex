\documentclass{sig-alternate} 
\usepackage{mathptmx} % This is Times font
\newcommand{\ignore}[1]{}
\usepackage{fancyhdr}
\usepackage[normalem]{ulem}
\usepackage[hyphens]{url}
\setlength{\emergencystretch}{10pt}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{breakurl}
\usepackage{tabularx}
\usepackage{balance}
\usepackage[nocompress]{cite}
\usepackage{subfig}
\usepackage{hyphenat}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{dcolumn}
\newcolumntype{d}{D{.}{.}{2.1}}
\usepackage{pifont}
%\usepackage{flushend}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{siunitx}
\newcommand*\mycirc[1]{{\large \ding{\numexpr201+#1\relax}}}
% Always include hyperref last
% \usepackage[bookmarks=true,breaklinks=true,letterpaper=true,colorlinks,linkcolor=black,citecolor=blue,urlcolor=black]{hyperref}
% required to break long URLS sanely
% \PassOptionsToPackage{hyphens}{url} \usepackage{hyperref}
\expandafter\def\expandafter\UrlBreaks\expandafter{\UrlBreaks% save the current one 
\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j% 
\do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t% 
\do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D% 
\do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N% 
\do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X% 
\do\Y\do\Z\do\*\do\-\do\~\do\'\do\"\do\-}%

% Ensure letter paper
\pdfpagewidth=8.5in
\pdfpageheight=11in
\pagenumbering{arabic}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\newcommand{\microsubmissionnumber}{265}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
  \node[shape=circle,draw,inner sep=1pt] (char) {#1};}}

\title{NUMA-Aware GPUs\\Creating Virtual GPUs for Performance Scalability}

\author{
Ugljesa Milic$^{\mp\ddagger}$,
Oreste Villa$^{\dagger}$,
Evgeny Bolotin$^{\dagger}$,
Akhil Arunkumar$^{\dagger\star}$,\\\\
Eiman Ebrahimi$^{\dagger}$,
Aamer Jaleel$^{\dagger}$,
Alex Ramirez$^{\dagger\ast}$,
David Nellans$^{\dagger}$
\\\\
$^{\mp}$Universitat Polit\`ecnica de Catalinya (UPC), $^{\ddagger}$Barcelona Supercomputing Center (BSC), \\
$^{\dagger}$NVIDIA, $^{\star}$Arizona State University, and $^{\ast}$Google\\
}

\begin{document}

\maketitle
\pagestyle{plain}

\textbf{Summary.} With Moore's law limiting the transister growth of 
next-generation GPU compute accelerators,  GPU architects must
embrace multi-socket designs in order to significantly increase the core count and memory 
bandwidth of future GPUs. However, maintaining the expected uniform memory 
system performance (of a single-GPU) in multi-GPU systems without significant 
application re-factoring stands as a major impediment to adoption of multi-socket GPU
systems. As a result, we believe that within the next 10 
years, GPU vendors will be forced to adopt programmer transparent runtimes that 
aggregate multiple physically discrete GPUs to form a single large virtual 
GPU, while also attempting to minimize the NUMA effects inherent in multi-GPU designs.

In this work we investigate the impact that NUMA-effects will have on 
transparent multi-GPU systems and show that several changes are needed to 
both the GPU interconnect and cache architectures to achieve good performance 
scalability. We show that application phase effects can be exploited allowing 
individual GPU sockets to dynamically optimize their individual interconnect and cache 
policies, minimizing the impact of NUMA effects. Our programmer transparent NUMA-aware 
GPU\footnote{\textbf{Original version appears in MICRO 2017:} \\ Ugljesa Milic, 
Oreste Villa, Evgeny Bolotin, Akhil Arunkumar, Eiman Ebrahimi, Aamer Jaleel, 
Alex Ramirez, David Nellans. "Beyond the Socket: NUMA-Aware GPUs", in 
proceedings of 2017 IEEE/ACM International Symposium on Microarchitecture (MICRO 
2017)} outperforms a single GPU by 1.5$\times$, 2.3$\times$, and 3.2$\times$ 
while achieving 89\%, 84\%, and 76\% of theoretical application scalability in 
2, 4, and 8 sockets designs respectively. Transparent multi-GPU systems are 
implementable using today's GPUs, and when combined with our proposed 
NUMA-aware architectural designs, will provide an easily adopted path to 
performance scalability for end-users.

\textbf{Motivation and Prior Work.} Over the past 10 years, GPUs have become a 
significant component in datacenter, high performance computing (HPC), and 
machine learning installations; improving the performance of these workloads by 
exploiting the abundant data parallelism. Nevertheless, with GPUs nearing the 
reticle limitation of maximum die size and the transistor density growth rate 
slowing down, developers looking to scale the performance of their single GPU 
programs are in a precarious position. Today, multi-GPU programming models do 
support explicit programming of two or more GPUs, but it is challenging to 
manage multiple GPUs and requires significant modification of traditional single 
GPU applications.  At the same time, GPUs are expanding beyond the traditional 
PCIe peripheral interface to enable more efficient interconnection protocols 
between both GPUs and CPUs. Future high bandwidth GPU-to-GPU interconnects, 
possibly using improved communication protocols, will lead to system designs 
with closely coupled groups of GPUs that can efficiently share memory at fine 
granularity.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{figures/inter_gpu_connections.pdf}
	\caption{The evolution of GPUs from traditional discrete PCIe devices to 
		single logical, multi-socket accelerators.}
	\vspace{-0.6cm}
	\label{fig:systemdiagram}
\end{figure}

The onset of such multi-socket GPU systems will provide a pivot point for GPU 
and system vendors. On one hand, vendors can continue to expose GPUs as 
individual resources and force developers to use multiple programming paradigms 
to leverage these GPUs. On the other, vendors can expose multi-socket designs as 
a single programmer transparent NUMA-GPU resource as shown in 
Figure~\ref{fig:systemdiagram}.  By extending the single GPU programming model 
to multi-socket GPUs, applications can scale beyond the constraints of Moore's 
law, while simultaneously retaining the programming interface which GPU 
developers have become accustomed.  Extracting significant performance 
scalability while maintaining existing programming interfaces is a desireable 
combination of traits that that will see widespread adoption if well executed by 
HW and SW architects.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=1.0\textwidth]{figures/plot_final_speedup_WB_nvlink_first.pdf}
	\caption{Final NUMA-aware GPU performance compared to a single GPU and 4$\times$ 
	larger single GPU with scaled resources.}
	\vspace{-0.2in}
	\label{fig:combined}
\end{figure*}

Researchers have previously examined aggregating multiple GPUs together under a single 
programming model~\cite{lee2013transparent,Cabezas2015}; however this work was 
done in an era where GPUs had limited memory addressability and relied on high 
latency, low bandwidth CPU-based PCIe interconnects. As a result, prior work 
was able to improve the multi-GPU programming experience but was unable to achieve
performance scalability. Building upon this work, we show that in a new
era of unified CPU and GPU memory~\cite{UVM}, cache line addressable high 
bandwidth interconnects~\cite{NVLINK}, and dedicated GPU and CPU socket PCB 
designs, scalable transparent multi-GPU performance is now achievable under 
existing single GPU programming models.

\vspace{-.05in}
\section{NUMA-aware GPU Designs}

\textbf{Dynamically Reconfigurable Interconnects:} In NUMA-GPU systems, we 
observe that applications have different utilization of egress and ingress 
channels on both a per GPU-socket basis, and during different execution phases. 
To increase effective NUMA-bandwidth (the primary limiting factor to NUMA-GPU 
performance), we propose to dynamically rebalance multi-lane link assignments, 
per-GPU basis. We suggest reallocating unidirectional links with bi-directional 
lanes which has an approximate 1\% area overhead on modern GPUs. During kernel 
execution the link load balancer periodically samples the BW saturation of each 
link. If the lanes in one direction are underutilized, while the lanes in the 
opposite direction are saturated, the link load balancer reconfigures and 
reverses the direction of one of the unsaturated lanes after quiescing all 
packets on that lane. This sample and reconfigure process stops only when 
directional utilization is not oversubscribed or all but one lane is configured 
in a single direction.

\textbf{NUMA-Aware Cache Partitioning:} To improve performance in situations 
where dynamic link balancing is ineffective (both directions saturated), 
decreasing the amount of traffic that crosses the low bandwidth NUMA channels is 
the only other option. GPU's today typically utilize memory-side L2 caches as a 
bandwidth filter to DRAM, but in NUMA-GPUs this is a bad architectural decision. 
Because memory-side caches only cache accesses that originate in their local 
memory, they cannot cache data from other NUMA zones and reduce NUMA 
interconnect traffic. To minimize inter-GPU bandwidth in multi-socket GPU 
systems we propose repurposing L2 cache capacity and instead moving using 
NUMA-aware cache partitioning algorithm applied to both L1 and L2 caches. This 
cache partitioning policy relies on the same link monitoring hardware as our 
adaptive link policy, but responds by repurposing cache capacity between local 
and remote memory to maximize performance.

\textbf{Performance Results: }Figure~\ref{fig:combined} shows the overall 
improvement NUMA-aware GPUs can achieve when applying our NUMA-aware policies in 
parallel. Our baseline is a 4-socket SW locality-optimized GPU with contiguous 
thread block scheduling and first touch page migration~\cite{Arunkumar2017}. We 
observe that on the right side of the graph, some workloads can achieve or 
surpass the case of a hypothetical 4$\times$ larger monolithic GPU. However, the 
applications on the left side show a large gap between the baseline NUMA design 
and theoretical performance that NUMA-aware GPU architectures can improve upon.  
We observe that both optimizations are complimentary and needed to maximize 
performance across a wide range of workloads. On average, we observe that, when 
combined, we see 2.1$\times$ improvement over a single GPU and 80\% improvement 
over a 4 socket multi-GPU without a NUMA optimized architecture.

\textit{NUMA-Aware transparent multi-GPU technology is most interesting because 
it allows users to easily scale performance by simply plugging in additional 
GPUs.} We show that on a verage a dual-socket NUMA GPU achieves 1.5$\times$ 
speedup, while 4 sockets and 8 sockets achieve 2.3$\times$ and 3.2$\times$ 
speedups respectively over a single GPU. Comparing our NUMA-aware GPU 
implementation to the scaling that applications could achieve on a hypothetical 
(but unbuildable) large single GPU, we see that NUMA GPUs can achieve 89\%, 
84\%, and 76\% the efficiency of a hypothetical single large GPU in 2, 4, and 8 
socket configurations respectively. This high efficiency factor indicates that 
our design is able to largely eliminate the NUMA penalty in future multi-socket 
GPU designs, though further improvements are possible.

\vspace{-.05in}
\section{Long Term Impact} Over the last decade single GPU performance has 
scaled thanks to a significant growth in per-GPU transistor count and DRAM 
bandwidth. Unfortunately, transistor density is slowing significantly and 
integrated circuit manufacturers are not providing roadmaps beyond \SI{7}{nm}.
Porting applications to multi-GPU programming models is difficult and presents
a significant barrier to adoption for many GPU-compute users.  This work provides
the blueprint for an easily achievable way to enable unmodified workload performance
scaling from one to as many as eight GPUs.
Today, multi-GPU compute nodes are already available: NVIDIA's DGX-1 
system with 8 GPUs and the US Department of Energy Summit and Sierra HPC 
nodes with 6 GPUs. However, only a small subset of applications, 
already parallelized with MPI benefit from an increased number of GPUs within a compute
node. Our NUMA-aware transparent multi-GPU design allows the code of many accelerated 
workloads to immediately strong-scale on multiple GPUs.\@

For decades, the promise of parallel code has been performance scalability. 
This has been realized on single GPUs, but today that promise does not hold on 
well on multilple GPUs, because crossing the socket boundary with efficient memory referencing 
was not well handled until recently. By combining dynamic page placement into memory at 
runtime~\cite{UVM} and interconnects with cache line granularity~\cite{NVLINK}, 
our proposal enables transparent access to remote memory without the need to 
modify application source code, extending the promise of parallel programming across
the socket boundary.  While other techniques are being proposed to improve the transistor
count of single GPUs such as multi-chip-modules GPUs~\cite{Arunkumar2017}, those
approaches do not have the same level of scalability as a transparent multi-GPU
system which can scale to 4,8, or 16 times larger than today's biggest GPUs if
the NUMA overheads can be overcome or hidden through microarchitectural improvements.
At the limit, future GPU-centric compute nodes are likely to be comprised of multiple-sockets
of multi-chip-module GPUs that have been made NUMA-aware at the architectural level, achieving
the best of both on-package and off-package scalability.  Performance
improvements that are 2-8 times beyond current GPUs will not be achieved through traditional
architectural improvements, big picture system architecture is where significant gains
will be made.

This work will be impactful both within the field of computer architecture, but 
more importantly outside the field of computer architecture.  Within our 
specialty, transparent multi-socket GPU execution will enable a wave of research 
similar to the 25 year (and on going) trend of multi-socket CPU compute 
research. While similar concepts are likely to be re-applied to the GPU domain, 
the NUMA bottleneck for GPUs is not a discrepancy between local and remote 
memory access latency but instead bandwidth;  instead of exploiting instruction 
level parallelism to hide the memory wall, it must now be done with energy-efficient 
use of data and thread parallelism while considering locality as a first class principle. 
There are many challenging problems and 
open questions (thread block scheduling, dynamic page migration and replication, 
prefetching policies, making other parts of GPU architecture NUMA-aware, etc.) 
that must be studied by a research community willing to address them.

Outside our field,  GPUs are the preeminant compute acceleration technique used in the fields
of machine learning, computer vision, and autonomous robotics.  The algorithms
used in these fields iterate at such a rapid pace, deep learning frameworks (for example)
have little time to heavily optimize for a single piece, or even generation, of GPU hardware
before new techniques are invented.  This means that most machine learning frameworks
are not yet multi-GPU capable, despite the best efforts of GPU vendors. NUMA-aware
transparent multi-GPUs can provide a immediate shot in the arm to machine learning researchers
across the globe by providing a 2-8$\times$ improvement in both GPU memory capacity and
throughput, while requiring no modification to their rapidly iterating code-bases.
We believe no technique known in the architecture community today has the same potential
upside, with such little overhead to the programmer, and can be realized in such a short timeline
as transparent NUMA-aware GPUs.  \textbf{Citation:} \textit{In 2017 Milic et al. provided a blueprint for NUMA-Aware
multi-socket GPUs and demonstrated that thanks to their inherently parallel design both at
the language and architectural level, unmodified GPU applications could see N-fold improvements 
in performance simply by increasing the number of discrete GPUs within the system.}

\bibliographystyle{IEEEtran}
\vspace{-0.05in}
\bibliography{main}
\end{document}
